{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LitMatter DeepChem\n",
    "* This notebook shows how to speed up [DeepChem](https://github.com/deepchem/deepchem) model training on [MoleculeNet](https://arxiv.org/abs/1703.00564) datasets using the LitMatter template.  \n",
    "* In this example, we train a simple DeepChem `TorchModel` on the Tox21 dataset.\n",
    "* The training workflow shown here can be scaled to hundreds of GPUs by changing a single keyword argument!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deepchem as dc\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning import (LightningDataModule, LightningModule, Trainer,\n",
    "                               seed_everything)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a `LitMolNet` dataset\n",
    "Any MolNet dataset from `deepchem.molnet` can be used with LitMatter. The specific MolNet dataset and any pre-processing steps can be defined in `data.LitMolNet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [11:14:30] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:14:30] WARNING: not removing hydrogen atom without neighbors\n",
      "RDKit WARNING: [11:14:41] WARNING: not removing hydrogen atom without neighbors\n",
      "[11:14:41] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    }
   ],
   "source": [
    "from lit_data.molnet_data import LitMolNet\n",
    "\n",
    "dm = LitMolNet(loader=dc.molnet.load_tox21, batch_size=16)\n",
    "dm.prepare_data()\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate a `LitDeepChem` model\n",
    "Any `deepchem.models.torch_models.TorchModel` can be used with LitMatter. Here, we'll write our own custom base model in PyTorch and make a `TorchModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function Model.__del__ at 0x7f028f6c6550>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/gridsan/NA30490/.conda/envs/litmatter/lib/python3.8/site-packages/deepchem/models/models.py\", line 61, in __del__\n",
      "    shutil.rmtree(self.model_dir)\n",
      "  File \"/home/gridsan/NA30490/.conda/envs/litmatter/lib/python3.8/shutil.py\", line 709, in rmtree\n",
      "    onerror(os.lstat, path, sys.exc_info())\n",
      "  File \"/home/gridsan/NA30490/.conda/envs/litmatter/lib/python3.8/shutil.py\", line 707, in rmtree\n",
      "    orig_st = os.lstat(path)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/state/partition1/slurm_tmp/48690281.0.0/tmp60c4fwyb'\n"
     ]
    }
   ],
   "source": [
    "from lit_models.deepchem_models import LitDeepChem\n",
    "\n",
    "base_model = torch.nn.Sequential(\n",
    "torch.nn.Linear(1024, 256),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 12),\n",
    ")\n",
    "\n",
    "torch_model = dc.models.TorchModel(base_model, loss=torch.nn.MSELoss())\n",
    "\n",
    "model = LitDeepChem(torch_model, lr=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "Simply change the `Trainer` flags as desired for multi-gpu and multi-node training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(gpus=-1,  # use all available GPUs on each node\n",
    "#                   num_nodes=1,  # change to number of available nodes\n",
    "#                  accelerator='ddp',\n",
    "                 max_epochs=5,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/NA30490/.conda/envs/litmatter/lib/python3.8/site-packages/pytorch_lightning/core/datamodule.py:469: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [GPU-13636255-d3c9-b0ac-83c7-b25c82e0dbc5]\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | model   | Sequential | 265 K \n",
      "1 | loss_fn | MSELoss    | 0     \n",
      "---------------------------------------\n",
      "265 K     Trainable params\n",
      "0         Non-trainable params\n",
      "265 K     Total params\n",
      "1.062     Total estimated model params size (MB)\n",
      "/home/gridsan/NA30490/.conda/envs/litmatter/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /home/gridsan/NA30490/litmatter_dev/litmatter/lightning_logs/version_48690281/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation sanity check:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/NA30490/.conda/envs/litmatter/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:56: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 16. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 392it [00:01, 225.99it/s, loss=14.3, v_num=4.87e+7]        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/NA30490/.conda/envs/litmatter/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:56: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: : 401it [00:01, 215.30it/s, loss=14.3, v_num=4.87e+7]\n",
      "Epoch 0: : 442it [00:01, 223.67it/s, loss=14.3, v_num=4.87e+7, val_loss=19.20]\n",
      "Epoch 1: : 0it [00:00, ?it/s, loss=14.3, v_num=4.87e+7, val_loss=19.20]       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gridsan/NA30490/.conda/envs/litmatter/lib/python3.8/site-packages/pytorch_lightning/utilities/data.py:56: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 15. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "  warning_cache.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: : 392it [00:01, 233.84it/s, loss=9.79, v_num=4.87e+7, val_loss=19.20, train_loss=13.20]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: : 442it [00:01, 235.14it/s, loss=9.79, v_num=4.87e+7, val_loss=19.00, train_loss=13.20]\n",
      "Epoch 2: : 392it [00:01, 264.65it/s, loss=13.7, v_num=4.87e+7, val_loss=19.00, train_loss=11.00]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: : 442it [00:01, 260.55it/s, loss=13.7, v_num=4.87e+7, val_loss=18.90, train_loss=11.00]\n",
      "Epoch 3: : 392it [00:01, 261.71it/s, loss=7.03, v_num=4.87e+7, val_loss=18.90, train_loss=8.930]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: : 442it [00:01, 259.93it/s, loss=7.03, v_num=4.87e+7, val_loss=19.30, train_loss=8.930]\n",
      "Epoch 4: : 392it [00:01, 268.69it/s, loss=6.4, v_num=4.87e+7, val_loss=19.30, train_loss=7.330] \n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: : 442it [00:01, 267.11it/s, loss=6.4, v_num=4.87e+7, val_loss=19.30, train_loss=7.330]\n",
      "Epoch 4: : 442it [00:01, 264.71it/s, loss=6.4, v_num=4.87e+7, val_loss=19.30, train_loss=7.330]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, datamodule=dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! By changing the `num_nodes` argument, training can be distributed across all available GPUs. For longer training jobs on an HPC cluster, see the provided example batch scripts."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-litmatter]",
   "language": "python",
   "name": "conda-env-.conda-litmatter-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
